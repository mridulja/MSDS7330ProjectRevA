{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import string\n",
    "import operator \n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import os\n",
    "import tweepy\n",
    "import twitter\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "import time\n",
    "from tweepy import OAuthHandler\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Establishing the database connection to MongoDb Database for example \"twitter_db\"\n",
    "try:\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    db = client['stream_apple']\n",
    "    collection = db['stream_appleCollection']\n",
    "except BaseException, e:\n",
    "    print str(e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 documents in users collection\n"
     ]
    }
   ],
   "source": [
    "# Find out how many tweets (documents) are in users collection, efficiently \n",
    "tweetcount = db.twitter_collection.find().count()\n",
    "print \"There are %d documents in users collection\" % tweetcount "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_iterator = collection.find()\n",
    "for tweet in tweets_iterator:\n",
    "    try:\n",
    "        tweet_text = ( tweet['text'])\n",
    "        user_screen_name = tweet['user']['screen_name']\n",
    "        user_name = tweet['user']['name']\n",
    "        retweet_count = tweet['retweeted_status']['retweet_count']\n",
    "        retweeted_name = tweet['retweeted_status']['user']['name']\n",
    "        retweeted_screen_name = tweet['retweeted_status']['user']['screen_name']\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_iterator = collection.find()\n",
    "retweets = [\n",
    "            # Store out a tuple of these three values ...\n",
    "            (status['retweeted_status']['retweet_count'], \n",
    "             status['retweeted_status']['user']['screen_name'],\n",
    "             status['text']) \n",
    "            \n",
    "            # ... for each status ...\n",
    "            for status in tweets_iterator \n",
    "            \n",
    "            # ... so long as the status meets this condition.\n",
    "                if status.has_key('retweeted_status')\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+----------------------------------------------------+\n",
      "| Count | Screen Name     | Text                                               |\n",
      "+-------+-----------------+----------------------------------------------------+\n",
      "| 64333 | Real_Liam_Payne | RT @Real_Liam_Payne: Check out what happened at    |\n",
      "|       |                 | our London Session https://t.co/zr9uOhel9Q         |\n",
      "| 64332 | Real_Liam_Payne | RT @Real_Liam_Payne: Check out what happened at    |\n",
      "|       |                 | our London Session https://t.co/zr9uOhel9Q         |\n",
      "| 59942 | Real_Liam_Payne | RT @Real_Liam_Payne: Check out what happened at    |\n",
      "|       |                 | our London Session https://t.co/zr9uOhvWyq         |\n",
      "| 59941 | Real_Liam_Payne | RT @Real_Liam_Payne: Check out what happened at    |\n",
      "|       |                 | our London Session https://t.co/zr9uOhvWyq         |\n",
      "| 48902 | Adamstone341    | RT @Adamstone341: Hey Adam! Next time you log into |\n",
      "|       |                 | your Twitter at the Apple Store make sure you log  |\n",
      "|       |                 | out haha! From Cindy http://t.co/NaeBI‚Ä¶            |\n",
      "| 48902 | Adamstone341    | RT @Adamstone341: Hey Adam! Next time you log into |\n",
      "|       |                 | your Twitter at the Apple Store make sure you log  |\n",
      "|       |                 | out haha! From Cindy http://t.co/NaeBI‚Ä¶            |\n",
      "| 5485  | YouAndBabe      | RT @YouAndBabe: He though he was getting an Apple  |\n",
      "|       |                 | Watch omg his reaction is amazing! üòç              |\n",
      "|       |                 | https://t.co/1XjIr4if4y                            |\n",
      "| 5484  | YouAndBabe      | RT @YouAndBabe: He though he was getting an Apple  |\n",
      "|       |                 | Watch omg his reaction is amazing! üòç              |\n",
      "|       |                 | https://t.co/1XjIr4if4y                            |\n",
      "| 2237  | GorillaAccounts | RT @GorillaAccounts: Want to win a FREE APPLE      |\n",
      "|       |                 | WATCH? Follow us and Retweet this for your chance  |\n",
      "|       |                 | to win! https://t.co/G4w5dnH8F1                    |\n",
      "| 2236  | GorillaAccounts | RT @GorillaAccounts: Want to win a FREE APPLE      |\n",
      "|       |                 | WATCH? Follow us and Retweet this for your chance  |\n",
      "|       |                 | to win! https://t.co/G4w5dnH8F1                    |\n",
      "+-------+-----------------+----------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Slice off the first 10 from the sorted results and display each item in the tuple\n",
    "\n",
    "pt = PrettyTable(field_names=['Count', 'Screen Name', 'Text'])\n",
    "[ pt.add_row(row) for row in sorted(retweets, reverse=True)[:10] ]\n",
    "pt.max_width['Text'] = 50\n",
    "pt.align= 'l'\n",
    "print pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to Pre-process the tweets\n",
    "# @-mentions, emoticons, URLs and #hash-tags are now preserved as individual tokens\n",
    "# We try to capture some emoticons, HTML tags, Twitter @usernames (@-mentions), \n",
    "# Twitter #hashtags, URLs, numbers, words with and without dashes and apostrophes, and finally ‚Äúanything else‚Äù\n",
    "import re\n",
    "\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Function to pretty print the JSON file we have collected in the previous function call\n",
    "#this function is just for testing purposes to see that the JSON data is loaded from the file\n",
    "tweets_iterator = collection.find()\n",
    "for tweet in tweets_iterator:\n",
    "    try:\n",
    "        tokens = preprocess(tweet['text'])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'iPhone', u'7', u'Release', u'Date', u'amp', u'Price', u'A10', u'Powered', u'Apple', u'Flagship', u'Pack', u'These', u'Specs', u'\\xa0', u'\\u2026', u'https://t.co/SWfAXbctQi']\n"
     ]
    }
   ],
   "source": [
    "#\"text\":\"RT @NFL: .@LukeKuechly Had a Day! #CARvsDAL https:\\/\\/t.co\\/MngpsMwpfz\"\n",
    "\n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via']\n",
    "terms_stop = [term for term in preprocess(tweet['text']) if term not in stop]\n",
    "print terms_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Apple', 214), (u'RT', 108), (u'#apple', 108), (u'#iphone', 106), (u'\\u2026', 94), (u'iPhone', 92), (u'GB', 70), (u'\\ud83d', 57), (u'Deals', 49), (u'16', 48)]\n",
      "[(u'#apple', 54), (u'#iphone', 53), (u'#', 29), (u'#Apple', 20), (u'#news', 18), (u'#fr', 15), (u'#france', 15), (u'#SixWordsStory', 7), (u'#MacBook', 7), (u'#Win', 5)]\n",
      "[(u'Apple', 428), (u'RT', 216), (u'\\u2026', 188), (u'iPhone', 184), (u'GB', 140), (u'\\ud83d', 114), (u'Deals', 98), (u'16', 96), (u'amp', 86), (u'5', 82)]\n"
     ]
    }
   ],
   "source": [
    "tweets_iterator = collection.find()\n",
    "count_all = Counter()\n",
    "count_all_hash = Counter()\n",
    "count_all_terms = Counter()\n",
    "for tweet in tweets_iterator:\n",
    "    try: \n",
    "        # Create a list with all the terms\n",
    "        terms_all = [term for term in preprocess(tweet['text']) if term not in stop]\n",
    "        # Update the counter\n",
    "        count_all.update(terms_all)\n",
    "        # Count terms only once, equivalent to Document Frequency\n",
    "        terms_single = set(terms_all)\n",
    "        # Count hashtags only\n",
    "        terms_hash = [term for term in preprocess(tweet['text']) if term.startswith('#')]\n",
    "        count_all_hash.update(terms_hash)\n",
    "        count_all.update(terms_hash)\n",
    "        # Count terms only (no hashtags, no mentions)\n",
    "        terms_only = [term for term in preprocess(tweet['text']) if term not in stop and not term.startswith(('#', '@'))] \n",
    "        count_all_terms.update(terms_only)\n",
    "        count_all_terms.update(terms_only)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Print the first 5 most frequent words\n",
    "print(count_all.most_common(10))\n",
    "print(count_all_hash.most_common(10))\n",
    "print(count_all_terms.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def per_minute(SearchKeyword):\n",
    "    dates_SearchKeyword = []\n",
    "    # f is the file pointer to the JSON data set\n",
    "    tweets_iterator = collection.find()\n",
    "    for tweet in tweets_iterator:\n",
    "        try:\n",
    "            # let's focus on hashtags only at the moment\n",
    "            terms_hash = [term for term in preprocess(tweet['text']) if term.startswith('#')]\n",
    "            # track when the hashtag is mentioned\n",
    "            if '#' + SearchKeyword in terms_hash:\n",
    "                dates_SearchKeyword.append(tweet['created_at'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # a list of \"1\" to count the hashtags\n",
    "    ones = [1]*len(dates_SearchKeyword)\n",
    "    # the index of the series\n",
    "    idx = pandas.DatetimeIndex(dates_SearchKeyword)\n",
    "    # the actual series (at series of 1s for the moment)\n",
    "    SearchKeyword = pandas.Series(ones, index=idx)\n",
    "\n",
    "    # Resampling / bucketing\n",
    "    per_minute_freq = SearchKeyword.resample('1Min', how='sum').fillna(0)\n",
    "    return per_minute_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "per_minute_CARvsDAL = per_minute('CARvsDAL')\n",
    "per_minute_Cowboys = per_minute('Cowboys')\n",
    "per_minute_Panthers = per_minute('Panthers')\n",
    "per_minute_TOUCHDOWN = per_minute('TOUCHDOWN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# all the data together\n",
    "match_data = dict(CARvsDAL=per_minute_CARvsDAL, Cowboys=per_minute_Cowboys, Panthers=per_minute_Panthers,\n",
    "                  TOUCHDOWN=per_minute_TOUCHDOWN)\n",
    "# we need a DataFrame, to accommodate multiple series\n",
    "all_matches = pandas.DataFrame(data=match_data,\n",
    "                               index=per_minute_CARvsDAL.index)\n",
    "# Resampling as above\n",
    "all_matches = all_matches.resample('1Min', how='sum').fillna(0)\n",
    "\n",
    "columns_list = ['CARvsDAL','Cowboys','Panthers', 'TOUCHDOWN']\n",
    "df = pandas.DataFrame(all_matches, columns=columns_list)\n",
    "plt.figure()\n",
    "df.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "com = defaultdict(lambda : defaultdict(int))\n",
    "tweets_iterator = collection.find()\n",
    "for tweet in tweets_iterator: \n",
    "    try:\n",
    "        terms_only = [term for term in preprocess(tweet['text']) \n",
    "                  if term not in stop \n",
    "                  and not term.startswith(('#', '@'))]\n",
    "    except:\n",
    "        pass\n",
    "    # Build co-occurrence matrix\n",
    "    for i in range(len(terms_only)-1):            \n",
    "        for j in range(i+1, len(terms_only)):\n",
    "            w1, w2 = sorted([terms_only[i], terms_only[j]])                \n",
    "            if w1 != w2:\n",
    "                com[w1][w2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'\\u0627', u'\\u0644'), 214), ((u'\\u0644', u'\\u064a'), 142), ((u'\\u0631', u'\\u0644'), 130), ((u'\\ud83d', u'\\ude02'), 124), ((u'\\u0648', u'\\u064a'), 105), ((u'\\u0430', u'\\u0435'), 101), ((u'\\u0645', u'\\u064a'), 92), ((u'\\u0646', u'\\u064a'), 89), ((u'\\u0628', u'\\u0644'), 78), ((u'\\u062c', u'\\u0644'), 76)]\n"
     ]
    }
   ],
   "source": [
    "com_max = []\n",
    "# For each term, look for the most common co-occurrent terms\n",
    "for t1 in com:\n",
    "    t1_max_terms = max(com[t1].items(), key=operator.itemgetter(1))[:10]\n",
    "    for t2 in t1_max_terms:\n",
    "        com_max.append(((t1, t2), com[t1][t2]))\n",
    "# Get the most frequent co-occurrences\n",
    "terms_max = sorted(com_max, key=operator.itemgetter(1), reverse=True)\n",
    "print(terms_max[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurrence for apple:\n",
      "[(u'apple', 34), (u'\\ud83d', 19), (u'\\ude02', 16), (u'I', 8), (u'like', 5), (u'pie', 5), (u'amp', 4), (u'RT', 4), (u'juice', 4), (u'\\u0142', 4), (u'good', 3), (u'\\u3046', 3), (u'tf', 2), (u'\\u8cb7', 2), (u'https://t.co/xS6w2Ph95w', 2), (u'cider', 2), (u'\\u3092', 2), (u'\\u304b', 2), (u'\\u3063', 2), (u'\\u306b', 2)]\n"
     ]
    }
   ],
   "source": [
    "search_word = \"apple\" # pass a search term\n",
    "count_search = Counter()\n",
    "tweets_iterator = collection.find()\n",
    "for tweet in tweets_iterator: \n",
    "    try:\n",
    "        terms_only = [term for term in preprocess(tweet['text']) if term not in stop and not term.startswith(('#', '@'))]\n",
    "        if search_word in terms_only:\n",
    "            count_search.update(terms_only)\n",
    "    except:\n",
    "        pass\n",
    "print(\"Co-occurrence for %s:\" % search_word)\n",
    "print(count_search.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((u'\\u0627', u'\\u0644'), (u'\\u0644', u'\\u064a'), (u'\\u0631', u'\\u0644'), (u'\\ud83d', u'\\ude02'), (u'\\u0648', u'\\u064a'), (u'\\u0430', u'\\u0435'), (u'\\u0645', u'\\u064a'), (u'\\u0646', u'\\u064a'), (u'\\u0628', u'\\u0644'), (u'\\u062c', u'\\u0644'), (u'Apple', u'GB'), (u'\\u062a', u'\\u0644'), (u'\\u0435', u'\\u043d'), (u'RT', u'\\u2026'), (u'16', u'GB'), (u'GB', u'iPhone'), (u'\\u0641', u'\\u0644'), (u'\\u062f', u'\\u064a'), (u'\\u304a', u'\\u30f3'), (u'\\u043e', u'\\u0440')) (214, 142, 130, 124, 105, 101, 92, 89, 78, 76, 72, 62, 50, 49, 49, 43, 40, 39, 38, 37)\n"
     ]
    }
   ],
   "source": [
    "#Plotting the most frequent words\n",
    "word_freq = terms_max[:20]\n",
    "labels, freq = zip(*word_freq)\n",
    "print labels, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot the top two words frequency as a bar chart\n",
    "n_groups = 20\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.35\n",
    "\n",
    "opacity = 0.4\n",
    "error_config = {'ecolor': '0.3'}\n",
    "\n",
    "rects1 = plt.bar(index, freq, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='b',\n",
    "                 error_kw=error_config,\n",
    "                 label='Top 2-words together')\n",
    "\n",
    "plt.xlabel('labels')\n",
    "plt.ylabel('freq')\n",
    "plt.xticks(index + bar_width, (labels))\n",
    "ax.set_xticklabels( labels, rotation=90, va='center' ) ;\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "retweet_threshold = 3\n",
    "tweets_iterator = collection.find()\n",
    "for tweet in tweets_iterator:\n",
    "    try: \n",
    "        # Create a list with all the terms\n",
    "        popular_tweets = [ status for status in tweet if status['retweet_count'] > retweet_threshold ] \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "for tweet in popular_tweets:\n",
    "    print tweet['text'], tweet['retweet_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "retweet_threshold = 20\n",
    "tweets_iterator = collection.find()\n",
    "for tweet in tweets_iterator:\n",
    "    # Create a list with all the terms\n",
    "    if tweet['retweeted_status.retweet_count'] > retweet_threshold:\n",
    "        print (tweet['text'], tweet['retweeted_status.retweet_count'])\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "retweet_threshold = 3\n",
    "tweets_iterator = collection.find()\n",
    "for tweetObj in tweets_iterator:\n",
    "    keys =  tweetObj.keys()\n",
    "    if 'retweet_count' in tweetObj and tweetObj['retweet_count'] >= retweet_threshold:\n",
    "        print tweetObj['text'] \n",
    "        print tweetObj['retweet_count']\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
